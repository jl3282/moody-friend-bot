{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import interpolate, stats\n",
    "\n",
    "CASE_DIR = 'data/CASE/data/interpolated'\n",
    "PHYS_DIR = os.path.join(CASE_DIR, 'physiological')\n",
    "ANN_DIR  = os.path.join(CASE_DIR, 'annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found subjects: ['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '4', '5', '6', '7', '8', '9', 'physiological']\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(PHYS_DIR)\n",
    "subs  = sorted({f.split('_')[1].split('.')[0] for f in files})\n",
    "print(\"Found subjects:\", subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid      = subs[0]  # change index to try different subjects\n",
    "physio_fp = os.path.join(PHYS_DIR, f'sub_{sid}.csv')\n",
    "annot_fp  = os.path.join(ANN_DIR,  f'sub_{sid}.csv')\n",
    "\n",
    "df_phys = pd.read_csv(physio_fp)\n",
    "df_ann  = pd.read_csv(annot_fp)\n",
    "\n",
    "print(f\"Subject {sid}:\")\n",
    "print(\"  physio shape:     \", df_phys.shape)\n",
    "print(\"  annotation shape: \", df_ann.shape)\n",
    "\n",
    "display(df_phys.head())\n",
    "display(df_ann.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy import interpolate, stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import os\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "import warnings\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CASE_DIR     = 'data/CASE/data/interpolated'\n",
    "PHYS_DIR     = os.path.join(CASE_DIR, 'physiological')\n",
    "ANN_DIR      = os.path.join(CASE_DIR, 'annotations')\n",
    "TARGET_FS    = 32    # Hz\n",
    "ORIG_FS      = 1000  # Hz\n",
    "WINDOW_SIZE  = 160   # 5 s @ 32 Hz\n",
    "STRIDE       = 32    # 1 s\n",
    "BATCH_SIZE   = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_subjects():\n",
    "    \"\"\"Return ['1','2',...,'30'] based on sub_<n>.csv filenames.\"\"\"\n",
    "    files = os.listdir(ANN_DIR)\n",
    "    subs = sorted({f.split('_')[1].split('.')[0] for f in files})\n",
    "    return subs\n",
    "\n",
    "def load_case_interpolated(subject_id: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read sub_<id>.csv from both physio & annotation folders,\n",
    "    assert they align, merge valence/arousal onto physio DataFrame.\n",
    "    \"\"\"\n",
    "    physio_fp = os.path.join(PHYS_DIR,     f'sub_{subject_id}.csv')\n",
    "    annot_fp  = os.path.join(ANN_DIR,      f'sub_{subject_id}.csv')\n",
    "    df_phys   = pd.read_csv(physio_fp)\n",
    "    df_ann    = pd.read_csv(annot_fp)\n",
    "\n",
    "    assert len(df_phys) == len(df_ann), \"Interpolation mismatch!\"\n",
    "    # pick only valence/arousal from annotation\n",
    "    df = df_phys.copy()\n",
    "    df['valence'] = df_ann['valence']\n",
    "    df['arousal'] = df_ann['arousal']\n",
    "    return df\n",
    "\n",
    "def downsample_to_32Hz(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Linearly resample every column (except 'video') from 1kHz â†’ 32Hz.\n",
    "    Drop 'daqtime', 'jstime', and keep only sensor + valence/arousal.\n",
    "    \"\"\"\n",
    "    # drop the time and video columns for interpolation\n",
    "    to_drop = [c for c in ('daqtime','jstime','video') if c in df.columns]\n",
    "    data = df.drop(columns=to_drop).values\n",
    "    orig_len, nchan = data.shape\n",
    "\n",
    "    new_len = int(orig_len * TARGET_FS / ORIG_FS)\n",
    "    t_orig  = np.linspace(0,1,orig_len)\n",
    "    t_new   = np.linspace(0,1,new_len)\n",
    "\n",
    "    out = np.zeros((new_len, nchan))\n",
    "    for i in range(nchan):\n",
    "        f = interpolate.interp1d(t_orig, data[:,i],\n",
    "                                 kind='linear',\n",
    "                                 fill_value='extrapolate')\n",
    "        out[:,i] = f(t_new)\n",
    "\n",
    "    return pd.DataFrame(out, columns=df.drop(columns=to_drop).columns)\n",
    "\n",
    "def create_windows(X: np.ndarray, y: np.ndarray,\n",
    "                   window_size=WINDOW_SIZE, stride=STRIDE):\n",
    "    Xw, yw = [], []\n",
    "    for start in range(0, len(X)-window_size+1, stride):\n",
    "        win = X[start:start+window_size]\n",
    "        lbls = y[start:start+window_size]\n",
    "        mode = stats.mode(lbls, keepdims=False).mode\n",
    "        Xw.append(win)\n",
    "        yw.append(mode)\n",
    "    return np.array(Xw), np.array(yw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '2',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '3',\n",
       " '30',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " 'annotations']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_subjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSData(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def prepare_case_dataloaders(test_size=0.3, random_state=42):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # 1) load & merge all subjects\n",
    "    all_X, all_y = [], []\n",
    "    for sid in list_subjects():\n",
    "        df = load_case_interpolated(sid)\n",
    "        df32 = downsample_to_32Hz(df)\n",
    "\n",
    "        # binarize valence into 2 classes (example)\n",
    "        v = df32['valence'].values\n",
    "        labels = (v > v.mean()).astype(int)\n",
    "\n",
    "        # build windows\n",
    "        Xw, yw = create_windows(df32.drop(columns=['valence','arousal']).values,\n",
    "                                labels)\n",
    "        all_X.append(Xw);  all_y.append(yw)\n",
    "\n",
    "    X = np.vstack(all_X)\n",
    "    y = np.hstack(all_y)\n",
    "\n",
    "    # 2) normalize\n",
    "    scaler = StandardScaler()\n",
    "    flat = X.reshape(-1, X.shape[2])\n",
    "    flat = scaler.fit_transform(flat)\n",
    "    X_norm = flat.reshape(X.shape)\n",
    "\n",
    "    # 3) split\n",
    "    X_tr, X_tmp, y_tr, y_tmp = train_test_split(X_norm, y,\n",
    "                                                test_size=test_size,\n",
    "                                                random_state=random_state,\n",
    "                                                stratify=y)\n",
    "    X_val, X_te, y_val, y_te = train_test_split(X_tmp, y_tmp,\n",
    "                                                test_size=0.5,\n",
    "                                                random_state=random_state,\n",
    "                                                stratify=y_tmp)\n",
    "\n",
    "    # 4) wrap\n",
    "    loaders = {}\n",
    "    for split, (X_s, y_s) in zip(\n",
    "        ['train','val','test'],\n",
    "        [(X_tr,y_tr),(X_val,y_val),(X_te,y_te)]\n",
    "    ):\n",
    "        ds = TSData(X_s, y_s)\n",
    "        loaders[split] = DataLoader(ds, batch_size=BATCH_SIZE,\n",
    "                                    shuffle=(split=='train'))\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Interpolation mismatch!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loaders \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_case_dataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, loader \u001b[38;5;129;01min\u001b[39;00m loaders\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m batches:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(loader), \n\u001b[1;32m      4\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m->\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(loader))[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m, in \u001b[0;36mprepare_case_dataloaders\u001b[0;34m(test_size, random_state)\u001b[0m\n\u001b[1;32m     14\u001b[0m all_X, all_y \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sid \u001b[38;5;129;01min\u001b[39;00m list_subjects():\n\u001b[0;32m---> 16\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mload_case_interpolated\u001b[49m\u001b[43m(\u001b[49m\u001b[43msid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     df32 \u001b[38;5;241m=\u001b[39m downsample_to_32Hz(df)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# binarize valence into 2 classes (example)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m, in \u001b[0;36mload_case_interpolated\u001b[0;34m(subject_id)\u001b[0m\n\u001b[1;32m     14\u001b[0m df_phys   \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(physio_fp)\n\u001b[1;32m     15\u001b[0m df_ann    \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(annot_fp)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df_phys) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(df_ann), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterpolation mismatch!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# pick only valence/arousal from annotation\u001b[39;00m\n\u001b[1;32m     19\u001b[0m df \u001b[38;5;241m=\u001b[39m df_phys\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mAssertionError\u001b[0m: Interpolation mismatch!"
     ]
    }
   ],
   "source": [
    "loaders = prepare_case_dataloaders()\n",
    "for name, loader in loaders.items():\n",
    "    print(f\"{name} batches:\", len(loader), \n",
    "            \"->\", next(iter(loader))[0].shape)\n",
    "# Now you can feed `loaders['train']`, etc. into your train_model/test_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moody",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
